{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms, utils\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubLayNetDataset(Dataset):\n",
    "    def __init__(self, root, new_size = (224, 224), resize=False):  # new_size format: (W,H)\n",
    "        self.root = root\n",
    "        self.new_size = new_size\n",
    "        self.resize = resize\n",
    "        # load all image files, sorting them\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Images\"))))\n",
    "        # .json file path\n",
    "        self.json = os.path.join(root, \"samples.json\")\n",
    "        \n",
    "        with open(self.json) as f:\n",
    "            self.templates = json.load(f)\n",
    "            f.close()\n",
    "        # images format: {pic id: {'file_name': '...', 'height': ..., 'width': ..., 'annotations': [{...}, ...]}}\n",
    "        self.images = {}\n",
    "        for image in self.templates['images']:\n",
    "            self.images[image['id']] = {'file_name': image['file_name'], 'height' : image['height'], \n",
    "                                        'width' : image['width'], 'annotations': []}\n",
    "        for ann in self.templates['annotations']:\n",
    "            self.images[ann['image_id']]['annotations'].append(ann)\n",
    "        #pictures id's\n",
    "        self.keys = list(self.images.keys())\n",
    "        \n",
    "     \n",
    "    def __getitem__(self, idx):\n",
    "        # load image\n",
    "        img_path = os.path.join(self.root, \"Images\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")        \n",
    "        # get objects\n",
    "        objects_key = 0\n",
    "        for key in self.keys:\n",
    "            if self.images[key]['file_name'] == self.imgs[idx]:\n",
    "                objects_key = key\n",
    "                break\n",
    "        # get image size\n",
    "        self.size = (self.images[objects_key]['width'], self.images[objects_key]['height'])\n",
    "        objects = self.images[objects_key]['annotations']\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for object_ in objects:\n",
    "            # get the label and bboxes\n",
    "            label = object_['category_id']\n",
    "            labels.append(label)\n",
    "            bbox = object_['bbox']\n",
    "            bbox_max_min = [min(bbox[0], bbox[2]), min(bbox[1], bbox[3]),  # [x_min, y_min, x_max, y_max]\n",
    "                              max(bbox[0], bbox[2]), max(bbox[1], bbox[3])]\n",
    "            boxes.append(bbox_max_min)        \n",
    " \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    " \n",
    "        image_id = torch.as_tensor([objects_key])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n",
    " \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.resize:\n",
    "            img, target = self._prepare_sample(img, target)\n",
    "            \n",
    "        transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        img = transform(img)\n",
    "        \n",
    "        return img, target\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    \n",
    "    def _prepare_sample(self, image, target):\n",
    "        image = image.resize(self.new_size, Image.ANTIALIAS)\n",
    "        boxes = target[\"boxes\"]\n",
    "        y_ratio = self.size[1] / self.new_size[1]\n",
    "        x_ratio = self.size[0] / self.new_size[0]\n",
    "        boxes[:, 0] /= x_ratio\n",
    "        boxes[:, 2] /= x_ratio\n",
    "        boxes[:, 1] /= y_ratio\n",
    "        boxes[:, 3] /= y_ratio\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        return np.array(image), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/media/kirb/ADATA HD680/examples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # replace the classifier with a new one, that has num_classes which is user-defined\n",
    "    num_classes = num_classes  # 5 class + background\n",
    " \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    " \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 6 classes: 5+background\n",
    "num_classes = 6\n",
    "# use our dataset and defined transformations\n",
    "publaynet = PubLayNetDataset(root, new_size=_, resize=False)\n",
    "\n",
    "# split the dataset in train and val set\n",
    "\n",
    "indices = torch.randperm(len(publaynet)).tolist()\n",
    "dataset = torch.utils.data.Subset(publaynet, indices[:16])\n",
    "dataset_val = torch.utils.data.Subset(publaynet, indices[16:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "# Num_when training models in jupyter notebook The workers parameter can only be 0, otherwise an error will occur, which is commented out here\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, # num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size, shuffle=False, # num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# SGD\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "# cos learning rate\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "\n",
    "# let's train it for   epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    # Engine.pyTrain_ofOne_The epoch function takes both images and targets. to(device)\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
    "\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # evaluate on the test dataset    \n",
    "    evaluate(model, data_loader_val, device=device)    \n",
    "    \n",
    "    print('')\n",
    "    print('==================================================')\n",
    "    print('')\n",
    "\n",
    "print(\"That's it!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
